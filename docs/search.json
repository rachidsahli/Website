[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rachid Sahli",
    "section": "",
    "text": "Je suis étudiant à l’IUT Paris Rives de Seine en Science des Données et alternant à l’INSEE. Au sein de la Direction des Statistiques Démographiques et Sociales, mes missions consistent à couvrir différentes bases de sondage et à apparier de gros volumes de données.\nJe travaille principalement avec R et Python, et j’ai créé un site de ressources appelé PratiqueR. Vous pouvez également me retrouver sur YouTube, où je partage des vidéos sur R et son environnement.\nEn dehors de mes études, je consacre mon temps libre à la construction de robots et je partage mes idées sur mon blog, Jiqiren.\nPassionné par les statistiques, la robotique 🦿🤖, le vélo 🚲, le cinéma 🎞️, et la programmation 👾, j’aime explorer et partager mes découvertes dans ces domaines.\nJe suis très curieux et toujours ouvert à de nouvelles idées. N’hésitez pas à me contacter pour discuter d’un projet ou explorer des opportunités de collaboration.\n\n\nFormation\n\n\n\n\n\n\nIUT de Paris - Rives de Seine (Université Paris Cité) | Paris, 75016\n\n\n\nBUT Science des données, parcours exploration et modélisation statistique | Sept 2022 - Juin 2025\nCours suivis : Statistique inférentielle, paramétrique et non-paramétrique, Modélisation statistique, Algèbre linéaire, Analyse, Probabilités, Machine learning, Data mining, Programmation, Base de données\n\n\n\n\nExpérience\n\n\n\n\n\n\nInstitut national de la statistique et des études économiques (INSEE) | Montrouge, 92120\n\n\n\nStatisticien | Sept 2023 - Sept 2025 Au sein de la Direction des statistiques démographiques et sociales, j’ai mené des travaux d’appariement de données administratives. L’objectif de ces travaux était de mesurer et de comparer la couverture de deux bases de sondages. Par ailleurs, j’ai comparé différents algorithmes d’appariement à l’aide d’analyses statistiques. De plus, j’ai mis en place des modèles de classification exploitant les données administratives afin de réaliser des prédictions."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "À propos",
    "section": "",
    "text": "IUT de Paris - Rives de Seine (Université Paris Cité) | Paris, France\n\n\n\nBachelor universitaire de technologie en Science des données | Sept 2022 - Juin 2025\nCours suivis : Algèbre, Statistique, Probabilités, Programmation, Base de données, Économie"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Trier par\n       Ordre par défaut\n         \n          Titre\n        \n         \n          Date - Le plus ancien\n        \n         \n          Date - Le plus récent\n        \n         \n          Auteur·rice\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nRégression logistique en pratique\n\n\n\n\n\n\nR\n\n\nMachine learning\n\n\n\nApplication d’un modèle de régression logistique pour prédire si un patient est atteint de diabète. Le modèle est également comparé à la méthode des \\(k\\) plus proches voisins.\n\n\n\n\n\n10 oct. 2024\n\n\n\n\n\n\n\n\n\n\n\n\nL’algorithme des \\(k\\) plus proches voisins\n\n\n\n\n\n\nR\n\n\nMachine learning\n\n\n\nApplication sur R de l’algorithme des \\(k\\) plus proches voisins.\n\n\n\n\n\n29 sept. 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCarte d’aide pour les Volontaires de Paris 2024\n\n\n\n\n\n\nR\n\n\nOpendata\n\n\n\nCarte interactive des volontaires de Paris 2024.\n\n\n\n\n\n9 sept. 2024\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "blog/knn.html",
    "href": "blog/knn.html",
    "title": "L’algorithme des \\(k\\) plus proches voisins",
    "section": "",
    "text": "Le but de l’apprentissage supervisé est de prévoir l’étiquette (classification) \\(Y\\) ou la valeur de \\(Y\\) (régression) associée à une nouvelle entrée \\(X\\), où il est sous-entendu que (\\(X,Y\\)) est une nouvelle réalisation des données, indépendante de l’échantillon observé.\nL’algorithme des \\(k\\) plus proches voisins est une méthode d’apprentissage supervisé. On peut l’utiliser pour classifier quand \\(Y_i\\) est une variable qualitative, les \\(Y_i\\) sont appelés étiquettes. On peut également l’utiliser pour prédire si \\(Y_i \\in \\mathbb{R}\\). C’est donc une régression et les \\(Y_i\\) sont appelés variables à expliquer.\nRemarque : On parle d’apprentissage supervisé car pour chaque \\(X_i\\) de l’échantillon d’apprentissage on dispose de \\(Y_i\\), l’étiquette. Au contraire, on parlera d’apprentissage non-supervisé lorsque l’échantillon est simplement constitué des \\(X_i\\)."
  },
  {
    "objectID": "projets.html",
    "href": "projets.html",
    "title": "Projets",
    "section": "",
    "text": "Trier par\n       Ordre par défaut\n         \n          Titre\n        \n         \n          Date - Le plus ancien\n        \n         \n          Date - Le plus récent\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nProjets BUT SD\n\n\n1 min.\n\n\n\nScience des données\n\n\nStatistique\n\n\n\nVoici une liste de mes projets réalisées durant mes 3 années à l’Institut Universitaire de Technologie de Paris.\n\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "projets/projets_but/louvre.html",
    "href": "projets/projets_but/louvre.html",
    "title": "Nocturne au musée du Louvre",
    "section": "",
    "text": "Tous les vendredis, le musée du Louvre offre un moment de magie à ses visiteurs au milieu de ses collections. Au programme de ces nocturnes hebdomadaires, de nombreuses activités pour petits et grands. Le 24 novembre 2023, mes camarades de l’IUT et moi avons pu participer à l’une de ces soirées. Nous avons décidé de présenter certains tableaux du célèbre musée au filtre de l’IA. Au moment où cette technologie prend une place active dans notre société, avec par exemple ChatGPT, DALL-E ou encore Mistral AI, il est intéressant de pouvoir approfondir l’utilisation de ces outils permettant de générer des éléments en lien, ici, avec l’art, par exemple. De plus, ce sujet fascinant qu’est l’intelligence artificielle est plus ou moins en lien avec notre formation en science des données. Explorer ce domaine où se mêlent mathématiques, informatique et données était particulièrement captivant.\n\nQu’avons nous fait ?\nL’objectif final était de pouvoir le 23 novembre 2024, présenter une œuvre en rapport avec l’IA au visiteur de la nocturne. Pour cela, 2 mois, auparavant, nous avons décidé de choisir un sujet d’intelligence artificielle qui pouvait coïncider avec une œuvre du musée. Après de longues recherches passionnantes, nous avons choisi de présenter les GAN (Generative adversarial networks). C’est une classe d’algorithmes d’apprentissage non supervisé. Ils permettent de générer des images avec un fort degré de réalisme. Nous avons trouvé la technologie et les méthodes très intéressantes, d’autant plus qu’elles aboutissent à des applications concrètes, telles que la génération d’images artistiques. Cependant, ces algorithmes sont utilisés dans bien d’autres domaines tels que la médecine ou encore la finance. Mais concrètement, comment ça marche ?\n\n\nQu’est-ce qu’un GAN ?\nAfin de comprendre comment il fonctionne, on peut imaginer un jeu entre deux joueurs :\n\nL’artiste (Générateur) : L’objectif du joueur est de créer des images qui se rapprochent le plus de la réalité. Pour cela, il apprend de ses erreurs et réessaie en continu d’obtenir l’œuvre la plus proche du réel.\nLe juge (Discriminateur) : L’objectif de ce joueur est de vérifier si les œuvres réalisées par l’artiste peuvent paraître réelles ou si elles sont encore trop fausses. Pour cela, il compare les œuvres de l’artiste à des œuvres réelles faites par des peintres.\n\nTant que l’image n’est pas accepté par le juge, l’artiste continue à produire des œuvres. Voilà, le fonctionnement d’un GAN ou deux réseaux de neurones se font concurrence. De cette manière, il est possible de créer des images, des vidéos ou d’autres contenues de très bonnes qualités.\n\n\n\n\n\n\n\nSchéma GAN\n\n\n\n\nPrésentation au public\nNous avons décidé de présenter au public le travail d’Obvious. Ce collectif de chercheurs, d’artistes travaille avec des modèles d’apprentissage profond pour explorer le potentiel créatif de l’intelligence artificielle. Ils ont justement utilisé des GAN pour générer une famille de 11 tableaux (la famille Belamy). \nUn portrait a retenu notre attention, le portrait d’Edmond de Belamy. Ce tableau est une impression sur toile qui est rentrée dans l’histoire de l’art moderne. Cela, car c’est la première œuvre d’art produite par un logiciel d’intelligence artificielle à être présentée dans une salle des ventes. Pour couronner le tout, il a été vendu 432 500 dollars chez Christie’s le 25 octobre 2018.  De plus, ce tableau est assez troublant. Il est très difficile à première vue de déterminer qu’une machine a pu en être l’auteur.   Obvious à utiliser un GAN, en l’entraînant sur 15 000 portraits classiques réalisés entre le 14e et 20e siècle. L’algorithme devait donc produire un tableau en sortie qui serait très ressemblant aux portraits classiques. Nous avons décidé de comparer le portrait d’Edmond de Belamy à une œuvre du Louvre se trouvant dans la salle 846 de l’aile Richelieu du musée. C’est une peinture datant du 17e siècle réalisée par Jean Bray, peintre néerlandais. Les tableaux ont quelques points en commun : le fond noir, un homme au centre du tableau, le col blanc avec une veste noir.\n\nPortrait de BelamyPortrait de Jean de Bray\n\n\n\n\n\n\n\n\n\nPortrait d’Edmond de Belamy, Collectif Obvious\n\n\n\n\n\n\n\n\n\n\n\nPortrait d’homme, 1658\n\n\n\n\n\nLes visiteurs étaient agréablement surpris par le réalisme du portrait d’Edmond de Belamy, mais aussi par le prix de vente de l’œuvre. Ils pensaient pouvoir reconnaître la réalisation d’une IA."
  },
  {
    "objectID": "projets/projets_but/louvre.html#quavons-nous-fait",
    "href": "projets/projets_but/louvre.html#quavons-nous-fait",
    "title": "Nocturne au musée du Louvre",
    "section": "Qu’avons nous fait ?",
    "text": "Qu’avons nous fait ?\nL’objectif final était de pouvoir le 23 novembre 2024, présenter une œuvre en rapport avec l’IA au visiteur de la nocturne. Pour cela, 2 mois, auparavant, nous avons décidé de choisir un sujet d’intelligence artificielle qui pouvait coïncider avec une œuvre du musée. Après de longues recherches passionnantes, nous avons choisi de présenter les GAN (Generative adversarial networks). C’est une classe d’algorithmes d’apprentissage non supervisé. Ils permettent de générer des images avec un fort degré de réalisme. Nous avons trouvé la technologie et les méthodes très intéressantes, d’autant plus qu’elles aboutissent à des applications concrètes, telles que la génération d’images artistiques. Cependant, ces algorithmes sont utilisés dans bien d’autres domaines tels que la médecine ou encore la finance. Mais concrètement, comment ça marche ?"
  },
  {
    "objectID": "projets/projets_but/louvre.html#quest-ce-quun-gan",
    "href": "projets/projets_but/louvre.html#quest-ce-quun-gan",
    "title": "Nocturne au musée du Louvre",
    "section": "Qu’est-ce qu’un GAN ?",
    "text": "Qu’est-ce qu’un GAN ?\nAfin de comprendre comment il fonctionne, on peut imaginer un jeu entre deux joueurs :\n\nL’artiste (Générateur) : L’objectif du joueur est de créer des images qui se rapprochent le plus de la réalité. Pour cela, il apprend de ses erreurs et réessaie en continu d’obtenir l’œuvre la plus proche du réel.\nLe juge (Discriminateur) : L’objectif de ce joueur est de vérifier si les œuvres réalisées par l’artiste peuvent paraître réelles ou si elles sont encore trop fausses. Pour cela, il compare les œuvres de l’artiste à des œuvres réelles faites par des peintres.\n\nTant que l’image n’est pas accepté par le juge, l’artiste continue à produire des œuvres. Voilà, le fonctionnement d’un GAN ou deux réseaux de neurones se font concurrence. De cette manière, il est possible de créer des images, des vidéos ou d’autres contenues de très bonnes qualités.\n\n\n\n\n\n\n\nSchéma GAN"
  },
  {
    "objectID": "projets/projets_but/louvre.html#présentation-au-public",
    "href": "projets/projets_but/louvre.html#présentation-au-public",
    "title": "Nocturne au musée du Louvre",
    "section": "Présentation au public",
    "text": "Présentation au public\nNous avons décidé de présenter au public le travail d’Obvious. Ce collectif de chercheurs, d’artistes travaille avec des modèles d’apprentissage profond pour explorer le potentiel créatif de l’intelligence artificielle. Ils ont justement utilisé des GAN pour générer une famille de 11 tableaux (la famille Belamy). \nUn portrait a retenu notre attention, le portrait d’Edmond de Belamy. Ce tableau est une impression sur toile qui est rentrée dans l’histoire de l’art moderne. Cela, car c’est la première œuvre d’art produite par un logiciel d’intelligence artificielle à être présentée dans une salle des ventes. Pour couronner le tout, il a été vendu 432 500 dollars chez Christie’s le 25 octobre 2018.  De plus, ce tableau est assez troublant. Il est très difficile à première vue de déterminer qu’une machine a pu en être l’auteur.   Obvious à utiliser un GAN, en l’entraînant sur 15 000 portraits classiques réalisés entre le 14e et 20e siècle. L’algorithme devait donc produire un tableau en sortie qui serait très ressemblant aux portraits classiques. Nous avons décidé de comparer le portrait d’Edmond de Belamy à une œuvre du Louvre se trouvant dans la salle 846 de l’aile Richelieu du musée. C’est une peinture datant du 17e siècle réalisée par Jean Bray, peintre néerlandais. Les tableaux ont quelques points en commun : le fond noir, un homme au centre du tableau, le col blanc avec une veste noir.\n\nPortrait de BelamyPortrait de Jean de Bray\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPortrait d’homme, 1658\n\n\n\n\n\nLes visiteurs étaient agréablement surpris par le réalisme du portrait d’Edmond de Belamy, mais aussi par le prix de vente de l’œuvre. Ils pensaient pouvoir reconnaître la réalisation d’une IA."
  },
  {
    "objectID": "projets/but_sd.html",
    "href": "projets/but_sd.html",
    "title": "Projets BUT SD",
    "section": "",
    "text": "Etude statistique dans un essai clinique\n\n\n1 min.\n\n\n\nR\n\n\nStatistique\n\n\n\n\n\n\n\n1 déc. 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMigration de données : De SQL à NoSQL\n\n\n4 min.\n\n\n\nSQL\n\n\n\nCe projet vise à migrer des données d’un environnement SQL vers un environnement NoSQL pour une entreprise automobile.\n\n\n\n28 déc. 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNocturne au musée du Louvre\n\n\n4 min.\n\n\n\nIA\n\n\n\nPrésentation d’une œuvre générée par une IA aux visiteurs du Musée du Louvre.\n\n\n\n24 nov. 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSérie Temporelle : Production de charbon aux États-Unis\n\n\n1 min.\n\n\n\nR\n\n\nRShiny\n\n\nStatistique\n\n\n\nLa prévision de la production de charbon aux États-Unis est réalisée à partir de l’analyse de la saisonnalité des séries temporelles et de la modélisation via trois méthodes…\n\n\n\n19 janv. 2024\n\n\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "about.html#formation",
    "href": "about.html#formation",
    "title": "À propos",
    "section": "",
    "text": "IUT de Paris - Rives de Seine (Université Paris Cité) | Paris, France\n\n\n\nBachelor universitaire de technologie en Science des données | Sept 2022 - Juin 2025\nCours suivis : Algèbre, Statistique, Probabilités, Programmation, Base de données, Économie"
  },
  {
    "objectID": "about.html#expérience",
    "href": "about.html#expérience",
    "title": "À propos",
    "section": "Expérience",
    "text": "Expérience\n\n\n\n\n\n\nInstitut national de la statistique et des études économiques (INSEE) | Montrouge, France\n\n\n\nProgrammeur Statistique | 2023 - 2025 Au sein de la Direction des statistiques démographiques et sociales, j’ai mené des travaux d’appariement de données administratives. L’objectif de ces travaux était de mesurer et de comparer la couverture de deux bases de sondages. Par ailleurs, j’ai comparé différents algorithmes d’appariement à l’aide d’analyses statistiques.\nMes missions ont inclus la manipulation de données brutes, le nettoyage des données, l’appariement de grands volumes de données, ainsi que leur analyse statistique."
  },
  {
    "objectID": "about.html#compétences",
    "href": "about.html#compétences",
    "title": "À propos",
    "section": "Compétences",
    "text": "Compétences\n\n\n\n\n\n\nProgrammation\n\n\n\nPython | R | SQL | SAS | GIT\n\n\n\n\n\n\n\n\nLangue\n\n\n\nAnglais (B2) | Espagnol (B2) | Arabe (C2)"
  },
  {
    "objectID": "projets/pratiquer.html",
    "href": "projets/pratiquer.html",
    "title": "PratiqueR",
    "section": "",
    "text": "J’ai développer PratiqueR pour les raisons suivantes :"
  },
  {
    "objectID": "projets/projets_but/integration.html",
    "href": "projets/projets_but/integration.html",
    "title": "Intégration de données dans un data warehouse",
    "section": "",
    "text": "Ce projet a été réalisé dans le cadre du cours d’intégration de données en BUT SD (2 ème année).\nL’objectif de ce module était de nous initier au stockage de données en entreprise afin de permettre une approche décisionnelle. Nous avons vu des notions telles que le data warehouse, l’ETL, les systèmes d’information décisionnels, SQL…"
  },
  {
    "objectID": "projets/projets_but/integration.html#introduction",
    "href": "projets/projets_but/integration.html#introduction",
    "title": "Intégration de données dans un data warehouse",
    "section": "Introduction",
    "text": "Introduction\nUne entreprise française de e-commerce nous a sollicités pour analyser le comportement des utilisateurs sur sa plateforme. Dans un premier temps, nous avons équipé l’entreprise d’un système de stockage de données. Ensuite, nous avons mis en place un tableau de bord permettant de répondre à la problématique posée."
  },
  {
    "objectID": "projets/projets_but/integration.html#modèle-relationnel",
    "href": "projets/projets_but/integration.html#modèle-relationnel",
    "title": "Intégration de données dans un data warehouse",
    "section": "Modèle relationnel",
    "text": "Modèle relationnel\nAprès une étude approfondie de l’environnement et des besoins de l’entreprise, nous avons établi un modèle relationnel pertinent, adapté à l’analyse du comportement des utilisateurs sur la plateforme.\nNous avons choisi d’utiliser un modèle relationnel en flocon."
  },
  {
    "objectID": "projets/pratiquer.html#accédez-au-site",
    "href": "projets/pratiquer.html#accédez-au-site",
    "title": "PratiqueR",
    "section": "Accédez au site",
    "text": "Accédez au site\nDécouvrez PratiqueR ici"
  },
  {
    "objectID": "blog/knn.html#prédiction",
    "href": "blog/knn.html#prédiction",
    "title": "L’algorithme des \\(k\\) plus proches voisins",
    "section": "3.1 Prédiction",
    "text": "3.1 Prédiction\nNous allons maintenant manipuler le jeu de données Abalone, disponible ici. Ce dernier contient des informations sur les ormeaux. Ce sont des mollusques marins qui possèdent une seule coquille et qui habitent principalement dans les eaux froides des côtes. La valeur commerciale des ormeaux est étroitement liée à leur âge, qui est le principal critère utilisé pour estimer leur prix. Déterminez l’âge des ormeaux se fait à partir de leurs anneaux (rings). C’est une tâche généralement réalisée en laboratoire qui prend beaucoup de temps. Ainsi, notre objectif est de prédire leur âge (ici la taille de leurs anneaux) à l’aide des variables physiologiques dont nous disposons.\nAfin de pouvoir réaliser notre prédiction nous importons les packages suivants.\n\n# Import library ---------------\nlibrary(kknn)\nlibrary(Metrics)\nlibrary(ggplot2)\n\n\nLe package kknn est une implémentation de l’algorithme des k plus proches voisins. Il permet notamment de pondérer les plus proches voisins lors de la prédiction.\nLe package Metrics est conçu pour évaluer les performances des modèles prédictifs en calculant diverses mesures d’erreur et de précision. Il est particulièrement utile pour les tâches de régression et de classification, car il propose un ensemble de fonctions pour mesurer l’exactitude des prédictions. Nous l’utiliserons pour calculer l’erreur quadratique moyenne.\nLe package ggplot2 permet d’obtenir des visualisations graphiques plus poussées.\n\nEnsuite, nous importons notre jeu de données en nommant les colonnes.\n\n# Import data ---------------\n# Nom des colonnes\ncolnames = c(\"Sex\",\"Length\",\"Diameter\",\"Height\",\"Whole_weight\",\"Shucked_weight\",\n             \"Viscera_weight\",\"Shell_weight\",\"Rings\")\n\n# Import de la table\nabalone &lt;- read.table(\"data_blog/abalone.data\", header = TRUE, sep = \",\", col.names = colnames)\n\nIl contient 4 176 observations et 9 variables. Nous supprimons la variable Sex et les observations pour qui la taille (Height) est égale à 0.\n\n# Longeur du jeu de donnees\ndim(abalone)\n\n[1] 4176    9\n\n# Suppression de la variable Sex\nabalone &lt;- abalone[,-1]\n\n# Suppression des valeurs nul\nabalone &lt;- subset(abalone, Height!=0)\n\n# Aucune valeurs manquantes\nsapply(abalone, function(x) sum(is.na(x)))\n\n        Length       Diameter         Height   Whole_weight Shucked_weight \n             0              0              0              0              0 \nViscera_weight   Shell_weight          Rings \n             0              0              0 \n\n\nCi-dessous, un petit aperçu des données.\n\nhead(abalone, 4)\n\n  Length Diameter Height Whole_weight Shucked_weight Viscera_weight\n1   0.35    0.265  0.090       0.2255         0.0995         0.0485\n2   0.53    0.420  0.135       0.6770         0.2565         0.1415\n3   0.44    0.365  0.125       0.5160         0.2155         0.1140\n4   0.33    0.255  0.080       0.2050         0.0895         0.0395\n  Shell_weight Rings\n1        0.070     7\n2        0.210     9\n3        0.155    10\n4        0.055     7\n\n\n\n3.1.1 Analyse descriptive\nOn procède à une succinte analyse descritpive de notre jeu de données.\n\nsummary(abalone)\n\n     Length          Diameter         Height        Whole_weight   \n Min.   :0.0750   Min.   :0.055   Min.   :0.0100   Min.   :0.0020  \n 1st Qu.:0.4500   1st Qu.:0.350   1st Qu.:0.1150   1st Qu.:0.4421  \n Median :0.5450   Median :0.425   Median :0.1400   Median :0.8000  \n Mean   :0.5241   Mean   :0.408   Mean   :0.1396   Mean   :0.8291  \n 3rd Qu.:0.6150   3rd Qu.:0.480   3rd Qu.:0.1650   3rd Qu.:1.1538  \n Max.   :0.8150   Max.   :0.650   Max.   :1.1300   Max.   :2.8255  \n Shucked_weight   Viscera_weight    Shell_weight        Rings       \n Min.   :0.0010   Min.   :0.0005   Min.   :0.0015   Min.   : 1.000  \n 1st Qu.:0.1861   1st Qu.:0.0935   1st Qu.:0.1300   1st Qu.: 8.000  \n Median :0.3360   Median :0.1710   Median :0.2340   Median : 9.000  \n Mean   :0.3595   Mean   :0.1807   Mean   :0.2389   Mean   : 9.934  \n 3rd Qu.:0.5020   3rd Qu.:0.2530   3rd Qu.:0.3289   3rd Qu.:11.000  \n Max.   :1.4880   Max.   :0.7600   Max.   :1.0050   Max.   :29.000  \n\n\nOn observe ci-dessous la distribution de la variable Rings ainsi que la relation entre la longueur et la taille des ormeaux.\n\nggplot(abalone, aes(x = Rings)) +\n  geom_histogram(fill = \"blue\") +\n  labs(title = \"Distribution de Rings\", y = \"Fréquence\", x = \"Rings\") + \n  theme_minimal()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nggplot(abalone, aes(x = Length, y = Height)) +\n  geom_point(col = \"blue\", pch = 1) +\n  labs(title = \"Relation entre Height et Length\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n3.1.2 Prédicition de la variable Rings\nComme dans la partie précédente, nous commençons par créer deux sous-échantillons distincts (échantillon d’apprentissage et echantillon de test) à partir du jeu de données complet.\n\nN = round((80/100)*nrow(abalone)) # Calcul du nombre d'observations a sélectionner (80 %) \nidx1 &lt;- sample(1:nrow(abalone), size = N, replace = FALSE) # Tirage aleatoire des indices qu'on va sélectionner\ndataL &lt;- abalone[idx1,] # Construction du dataset d'apprentissage\ndataV &lt;- abalone[-idx1,] # Construction du dataset de test ou de validite\n\nA présent, on utilise la fonction kknn() pour mettre en oeuvre notre algorithme de prédiction en fixant \\(k = 3\\).\n\npred &lt;- kknn(Rings ~., dataL, dataV, k = 3, kernel = 'rectangular')\n\nCi-dessous, nous observons nos prédictions en fonction de la variable Rings.\n\nplot(dataV$Rings,pred$fitted.values, xlab = \"Rings\", ylab = \"Prediction\", col = \"blue\")\nabline(0,1, col = \"red\")\n\n\n\n\n\n\n\n\nContrairement à la classification, nous utiliserons l’erreur quadratique moyenne pour mesurer la performance de notre modèle sur l’échantillon de test.\n\nmse &lt;- mse(pred$fitted.values, dataV$Rings)\npaste0(\"Erreur quadratique moyenne = \",mse)\n\n[1] \"Erreur quadratique moyenne = 5.73666001330672\"\n\n\nEnfin, nous allons identifier la valeur de \\(k\\) pour laquelle l’erreur quadratique moyenne est la plus faible. On pourra alors déterminer le niveau optimal de \\(k\\) afin d’améliorer la précision du modèle. La boucle suivante permet de calculer l’erreur quadratique moyenne pour chaque valeur de \\(k\\) sur notre échantillon.\n\nkvec &lt;- 1:100\nerror &lt;- rep(NA, length(kvec))\n\nfor(i in 1:length(kvec)){\n  pred &lt;- kknn(Rings ~., dataL, dataV, k = i, kernel = 'rectangular')\n  error[i] &lt;- mse(dataV$Rings, pred$fitted.values)\n}\n\nOn visualise les résultats sur le graphique ci-dessous.\n\nplot(kvec, error, type = \"b\", col = \"orange\")\nmin_error_niveau &lt;- which.min(error)\nabline(v = kvec[min_error_niveau], col = \"red\", lty = 2)\nlegend(\"topright\", legend = paste(\"Min error at k =\", kvec[min_error_niveau]), col = \"red\", lty = 2)"
  },
  {
    "objectID": "blog/knn.html#analyse-descriptive",
    "href": "blog/knn.html#analyse-descriptive",
    "title": "L’algorithme des \\(k\\) plus proches voisins",
    "section": "Analyse descriptive",
    "text": "Analyse descriptive\nOn procède à une succinte analyse descritpive de notre jeu de données.\n\nsummary(abalone)\n\n     Length          Diameter         Height        Whole_weight   \n Min.   :0.0750   Min.   :0.055   Min.   :0.0100   Min.   :0.0020  \n 1st Qu.:0.4500   1st Qu.:0.350   1st Qu.:0.1150   1st Qu.:0.4421  \n Median :0.5450   Median :0.425   Median :0.1400   Median :0.8000  \n Mean   :0.5241   Mean   :0.408   Mean   :0.1396   Mean   :0.8291  \n 3rd Qu.:0.6150   3rd Qu.:0.480   3rd Qu.:0.1650   3rd Qu.:1.1538  \n Max.   :0.8150   Max.   :0.650   Max.   :1.1300   Max.   :2.8255  \n Shucked_weight   Viscera_weight    Shell_weight        Rings       \n Min.   :0.0010   Min.   :0.0005   Min.   :0.0015   Min.   : 1.000  \n 1st Qu.:0.1861   1st Qu.:0.0935   1st Qu.:0.1300   1st Qu.: 8.000  \n Median :0.3360   Median :0.1710   Median :0.2340   Median : 9.000  \n Mean   :0.3595   Mean   :0.1807   Mean   :0.2389   Mean   : 9.934  \n 3rd Qu.:0.5020   3rd Qu.:0.2530   3rd Qu.:0.3289   3rd Qu.:11.000  \n Max.   :1.4880   Max.   :0.7600   Max.   :1.0050   Max.   :29.000  \n\n\nOn observe ci-dessous la distribution de la variable Rings ainsi que la relation entre la longueur et la taille des ormeaux.\n\nggplot(abalone, aes(x = Rings)) +\n  geom_histogram(fill = \"blue\") +\n  labs(title = \"Distribution de Rings\", y = \"Fréquence\", x = \"Rings\") + \n  theme_minimal()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nggplot(abalone, aes(x = Length, y = Height)) +\n  geom_point(col = \"blue\", pch = 1) +\n  labs(title = \"Relation entre Height et Length\") +\n  theme_minimal()"
  },
  {
    "objectID": "blog/knn.html#prédicition-de-la-variable-rings",
    "href": "blog/knn.html#prédicition-de-la-variable-rings",
    "title": "L’algorithme des \\(k\\) plus proches voisins",
    "section": "Prédicition de la variable Rings",
    "text": "Prédicition de la variable Rings\nComme dans la partie précédente, nous commençons par créer deux sous-échantillons distincts (échantillon d’apprentissage et echantillon de test) à partir du jeu de données complet.\n\nN = round((80/100)*nrow(abalone)) # Calcul du nombre d'observations a sélectionner (80 %) \nidx1 &lt;- sample(1:nrow(abalone), size = N, replace = FALSE) # Tirage aleatoire des indices qu'on va sélectionner\ndataL &lt;- abalone[idx1,] # Construction du dataset d'apprentissage\ndataV &lt;- abalone[-idx1,] # Construction du dataset de test ou de validite\n\nA présent, on utilise la fonction kknn() pour mettre en oeuvre notre algorithme de prédiction en fixant \\(k = 3\\).\n\npred &lt;- kknn(Rings ~., dataL, dataV, k = 3, kernel = 'rectangular')\n\nCi-dessous, nous observons nos prédictions en fonction de la variable Rings.\n\nplot(dataV$Rings,pred$fitted.values, xlab = \"Rings\", ylab = \"Prediction\", col = \"blue\")\nabline(0,1, col = \"red\")\n\n\n\n\n\n\n\n\nContrairement à la classification, nous utiliserons l’erreur quadratique moyenne pour mesurer la performance de notre modèle sur l’échantillon de test.\n\nmse &lt;- mse(pred$fitted.values, dataV$Rings)\npaste0(\"Erreur quadratique moyenne = \",mse)\n\n[1] \"Erreur quadratique moyenne = 6.92375249500998\"\n\n\nEnfin, nous allons identifier la valeur de \\(k\\) pour laquelle l’erreur quadratique moyenne est la plus faible. On pourra alors déterminer le niveau optimal de \\(k\\) afin d’améliorer la précision du modèle. La boucle suivante permet de calculer l’erreur quadratique moyenne pour chaque valeur de \\(k\\) sur notre échantillon.\n\nkvec &lt;- 1:100\nerror &lt;- rep(NA, length(kvec))\n\nfor(i in 1:length(kvec)){\n  pred &lt;- kknn(Rings ~., dataL, dataV, k = i, kernel = 'rectangular')\n  error[i] &lt;- mse(dataV$Rings, pred$fitted.values)\n}\n\nOn visualise les résultats sur le graphique ci-dessous.\n\nplot(kvec, error, type = \"b\", col = \"orange\")\nmin_error_niveau &lt;- which.min(error)\nabline(v = kvec[min_error_niveau], col = \"red\", lty = 2)\nlegend(\"topright\", legend = paste(\"Erreur min à k =\", kvec[min_error_niveau]), col = \"red\", lty = 2)"
  },
  {
    "objectID": "projets/pratiquer.html#apprendre-r-pas-à-pas",
    "href": "projets/pratiquer.html#apprendre-r-pas-à-pas",
    "title": "PratiqueR",
    "section": "",
    "text": "J’ai créer PratiqueR pour les raisons suivantes :\n- Expliquer les bases du langage R de manière claire et progressive.\n- Illustrer des cas d’utilisation courants à l’aide d’exemples concrets et applicables.\n- Proposer des exercices pratiques pour renforcer vos compétences et faciliter la prise en main."
  },
  {
    "objectID": "projets/pratiquer.html#aperçu-du-site",
    "href": "projets/pratiquer.html#aperçu-du-site",
    "title": "PratiqueR",
    "section": "Aperçu du site",
    "text": "Aperçu du site"
  },
  {
    "objectID": "blog/carte_volontaires.html",
    "href": "blog/carte_volontaires.html",
    "title": "Carte d’aide pour les Volontaires de Paris 2024",
    "section": "",
    "text": "Lors des Jeux Olympiques de 2024 à Paris, plus de 45 000 volontaires ont été les véritables hommes et femmes de l’ombre. On les a vus partout à Paris grâce à leur tenue bleue. Ils ont contribué au succès des Jeux Olympiques. Venant des quatre coins de la France, ils devaient pouvoir se repérer. La carte d’aide pour les volontaires est conçue à cet effet et leur a été très utile durant leur séjour à Paris.\n\n\n\n\n\n\n Cette carte a été créée à l’aide du package leaflet, en utilisant les données disponibles en open data fournies par les acteurs suivants :\n\nFontaines à eau : opendata de la Ville de Paris\nToilettes publiques : opendata de la Ville de Paris\nDistributeurs automatiques de billets : Opendatasoft hub\nParkings vélo : opendata de Paris 2024\nSite de compétitions des Jeux Olympiques\nParalympiques : opendata de Paris 2024\n\n(Denière mis à jour des données le 27 juillet 2024)"
  },
  {
    "objectID": "blog/reg_logistique.html",
    "href": "blog/reg_logistique.html",
    "title": "Régression logistique en pratique",
    "section": "",
    "text": "Introduction\nCe billet de blog a pour objectif d’étudier le modèle de régression logistique et de le comparer ainsi à la méthode des \\(k\\) plus proches voisins. Ces deux méthodes sont des algorithmes d’apprentissage supervisé. Le but de l’apprentissage supervisé est de prévoir l’étiquette (classification) \\(Y\\) ou la valeur de \\(Y\\) (régression) associée à une nouvelle entrée \\(X\\), où il est sous-entendu que (\\(X,Y\\)) est une nouvelle réalisation des données, indépendante de l’échantillon observé.\nLa régression logistique est une méthode de classification. Ce modèle de classification binaire permet d’expliquer une variable (\\(Y\\)) par p variables explicatives (\\(X_1,...,X_j\\). La variable \\(Y\\) ne peut prendre que deux modalités \\(\\left\\{0 ;1\\right\\}\\). Les variables \\(X_j\\) sont exclusivement continues ou binaires (on re-code les variables qualitatives avec des 0 et 1).\nL’algorithme des \\(k\\) plus proches voisins fonctionne de la façon suivante pour la classification. On détermine les \\(k\\) plus proches \\(X_i\\) de l’échantillon par rapport à \\(X\\) et on attribue la modalité dominante parmi les \\(k\\) modalités observées (on parle de vote majoritaire).\nNous allons utiliser ces deux méthodes de classification afin de prédire, en se basant sur des mesures diagnostiques, si un patient est atteint du diabète ou non.\n\n\nImport et préparation des données\nDans un premier temps, nous importons le jeu de données “diabetes.csv”. Nous allons mettre en pratique nos méthodes de régression logistique et \\(k\\) plus proches voisins sur ce dernier. Ce jeu de données est issu de l’Institut national du diabète et des maladies digestives et rénales. Tous les patients ici sont des femmes d’au moins 21 ans d’origine Pima. Il contient les mesures suivantes.\n\nDescription des variables\n\n\n\n\n\n\n\nVariable\nDescription\nType\n\n\n\n\nPregnancies\nNombre de grossesses\nQuanti continue\n\n\nGlucose\nConcentration de glucose plasmatique\nQuanti discrète\n\n\nBloodPressure\nPression artérielle diastolique (mm Hg)\nQuanti discrète\n\n\nSkinThickness\nÉpaisseur du pli cutané du triceps (mm)\nQuanti discrète\n\n\nInsulin\nInsuline sérique à 2 heures (mu U/ml)\nQuanti discrète\n\n\nBMI\nIndice de masse corporelle (poids en kg / (taille en m)²)\nQuanti continue\n\n\nDiabetesPedigree\nFonction de prédisposition au diabète\nQuanti continue\n\n\nAge\nÂge (années)\nQuanti discrète\n\n\nOutcome\nStatut diabétique (oui ou non)\nQuanti discrète\n\n\n\n\n# Import library ---------------\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(class)\n\n# Import data ---------------\ndiabetes &lt;- read.csv(\"data_blog/diabetes.csv\")\n\ndiabetes$Outcome &lt;- as.factor(diabetes$Outcome)\n\nNous avons en même temps importé 3 packages qui nous seront utiles dans la réalisation de notre travail. À savoir, ggplot pour la réalisation de graphique, et class pour la classification de l’algorithme des \\(k\\) plus proches voisins.\nVoici un petit aperçu de nos données.\n\nhead(diabetes)\n\n  Pregnancies Glucose BloodPressure SkinThickness Insulin  BMI\n1           6     148            72            35       0 33.6\n2           1      85            66            29       0 26.6\n3           8     183            64             0       0 23.3\n4           1      89            66            23      94 28.1\n5           0     137            40            35     168 43.1\n6           5     116            74             0       0 25.6\n  DiabetesPedigreeFunction Age Outcome\n1                    0.627  50       1\n2                    0.351  31       0\n3                    0.672  32       1\n4                    0.167  21       0\n5                    2.288  33       1\n6                    0.201  30       0\n\n\nNotre jeu de données ne présente pas de valeurs manquantes (NA), mais contient des observations pour lesquelles la valeur est égale à 0. Nous considérerons ces valeurs comme manquantes et les supprimons. Nous faisons cela uniquement sur 5 variables pour qui l’on juge que la valeur 0 est une vraie donnée manquante. Par exemple, nous ne le faisons pas pour la variable Pregnancies pour qui le 0 veut tout simplement dire que la personne n’a jamais été enceinte. Nous conservons alors 392 individus sur les 768 initiaux.\n\n# Suppression des 0\ndiabetes &lt;- diabetes[diabetes$SkinThickness!=0,]\ndiabetes &lt;- diabetes[diabetes$Insulin!=0,]\ndiabetes &lt;- diabetes[diabetes$Glucose!=0,]\ndiabetes &lt;- diabetes[diabetes$BloodPressure!=0,]\ndiabetes &lt;- diabetes[diabetes$BMI!=0,] \n\nCette suppression à pour objectif de ne pas fausser les relations que nous cherchons à modéliser. Par exemple, avoir une insuline à 0 n’est premièrement pas cohérent et peut influencer de manière disproportionnée le modèle, car les autres observations sont éloignées. En somme, nous essayons de garantir que le modèle représente fidèlement la relation entre les variables.\n\n\nStatistique descriptive\nÀ présent, nous allons étudier nos données de manière descriptive afin d’en obtenir un aperçu et ainsi de visualiser la répartition de la variable \\(Y\\) (Outcome). De plus, nous analyserons les corrélations entre les co-variables elles-mêmes, ainsi qu’entre celles-ci et la variable \\(Y\\).\n\nsummary(diabetes)\n\n  Pregnancies        Glucose      BloodPressure    SkinThickness  \n Min.   : 0.000   Min.   : 56.0   Min.   : 24.00   Min.   : 7.00  \n 1st Qu.: 1.000   1st Qu.: 99.0   1st Qu.: 62.00   1st Qu.:21.00  \n Median : 2.000   Median :119.0   Median : 70.00   Median :29.00  \n Mean   : 3.301   Mean   :122.6   Mean   : 70.66   Mean   :29.15  \n 3rd Qu.: 5.000   3rd Qu.:143.0   3rd Qu.: 78.00   3rd Qu.:37.00  \n Max.   :17.000   Max.   :198.0   Max.   :110.00   Max.   :63.00  \n    Insulin            BMI        DiabetesPedigreeFunction      Age       \n Min.   : 14.00   Min.   :18.20   Min.   :0.0850           Min.   :21.00  \n 1st Qu.: 76.75   1st Qu.:28.40   1st Qu.:0.2697           1st Qu.:23.00  \n Median :125.50   Median :33.20   Median :0.4495           Median :27.00  \n Mean   :156.06   Mean   :33.09   Mean   :0.5230           Mean   :30.86  \n 3rd Qu.:190.00   3rd Qu.:37.10   3rd Qu.:0.6870           3rd Qu.:36.00  \n Max.   :846.00   Max.   :67.10   Max.   :2.4200           Max.   :81.00  \n Outcome\n 0:262  \n 1:130  \n        \n        \n        \n        \n\n\n\n\n\n\n\n\n\n\n\nDans notre échantillon, 262 patients (67 %) ne sont pas atteints de diabète, tandis que 130 patients (33 %) en sont atteints. Les personnes non-diabétiques sont donc largement plus nombreuses.\nEnviron 14,3 % des patients n’ont jamais eu de grossesse. Il y a 23,7 % des patients qui ont eu une unique grossesse, et nous constatons une diminution progressive du nombre de patients à mesure que le nombre de grossesses augmente. La majorité des patients ont eu au moins une grossesse, ce qui indique que les grossesses sont relativement courantes dans cet échantillon.\n\n\n\n\n\n\n\n\n\nOn observe ci-dessous, l’âge des patients. Le plus jeune patient a 21 ans, tandis que le plus âgé a 81 ans. Ce dernier se distingue clairement en haut de la boîte à moustaches, où il se situe assez éloigné des autres patients. On note également 5 autres valeurs aberrantes avec un âge très élevé. L’âge médian dans l’échantillon est de 29 ans. Il est proche du Q1, on peut donc penser à une représentation assez élevée de patients plutôt jeunes.\n\n\n\n\n\n\n\n\n\nÀ présent, nous observons les différentes corrélations entre nos variables quantitatives continues. L’objectif du coefficient de corrélation est de muserer la force de relation linéaire entre deux variables. Il se calcule de la manière suivante :\n\\[\nR(x, y) = \\frac{\\text{Cov}(x, y)}{\\sigma_x \\sigma_y}\n\\]\nLe coefficient de corrélation est compris entre -1 et 1 :\n\nSi \\(R(x,y)\\) est proche de 0 : La relation linéaire est nulle.\nSi \\(R(x,y)\\) est proche de -1 : La relation linéaire est forte mais négative.\nSi \\(R(x,y)\\) est proche de 1 : La relation linéaire est forte.\n\n\ncor(diabetes[,c(-1,-9)])\n\n                           Glucose BloodPressure SkinThickness   Insulin\nGlucose                  1.0000000     0.2100266     0.1988558 0.5812230\nBloodPressure            0.2100266     1.0000000     0.2325712 0.0985115\nSkinThickness            0.1988558     0.2325712     1.0000000 0.1821991\nInsulin                  0.5812230     0.0985115     0.1821991 1.0000000\nBMI                      0.2095159     0.3044034     0.6643549 0.2263965\nDiabetesPedigreeFunction 0.1401802    -0.0159711     0.1604985 0.1359058\nAge                      0.3436415     0.3000389     0.1677611 0.2170820\n                               BMI DiabetesPedigreeFunction        Age\nGlucose                  0.2095159               0.14018018 0.34364150\nBloodPressure            0.3044034              -0.01597110 0.30003895\nSkinThickness            0.6643549               0.16049853 0.16776114\nInsulin                  0.2263965               0.13590578 0.21708199\nBMI                      1.0000000               0.15877104 0.06981380\nDiabetesPedigreeFunction 0.1587710               1.00000000 0.08502911\nAge                      0.0698138               0.08502911 1.00000000\n\n\nLes variables sont toutes moyennement corrélées positivement.\nNous représentons ci-dessous les relations entre les co-variables et notre variable \\(Y\\) (Outcome).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn observe une corrélation entre nos variables quantitatives continues et la variable Outcome (\\(Y\\)). On le voit par exemple à travers le lien entre la variable Glucose et la variable \\(Y\\). Les patients atteints de diabète présentent un taux de glucose significativement plus élevé que ceux qui ne sont pas atteints par la maladie.\n\n\nModèle de regression logistique\nNous pouvons mettre en place notre modèle de régression logistique. Nous commençons par le modèle complet. C’est-à-dire que nous sélectionnons toutes les co-variables pour expliquer \\(Y\\). On rappel qu’on cherche à prédire les valeurs de la variable Outcom (\\(Y\\)) à l’aide des variables explicatives présentent dans notre jeu de données.\nOn utilise pour cela la fonction glm() (Fitting Generalized Linear Models).\n\nmodel_complet &lt;- glm(Outcome~., family = \"binomial\", data = diabetes)\nsummary(model_complet)\n\n\nCall:\nglm(formula = Outcome ~ ., family = \"binomial\", data = diabetes)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.7823  -0.6603  -0.3642   0.6409   2.5612  \n\nCoefficients:\n                           Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)              -1.004e+01  1.218e+00  -8.246  &lt; 2e-16 ***\nPregnancies               8.216e-02  5.543e-02   1.482  0.13825    \nGlucose                   3.827e-02  5.768e-03   6.635 3.24e-11 ***\nBloodPressure            -1.420e-03  1.183e-02  -0.120  0.90446    \nSkinThickness             1.122e-02  1.708e-02   0.657  0.51128    \nInsulin                  -8.253e-04  1.306e-03  -0.632  0.52757    \nBMI                       7.054e-02  2.734e-02   2.580  0.00989 ** \nDiabetesPedigreeFunction  1.141e+00  4.274e-01   2.669  0.00760 ** \nAge                       3.395e-02  1.838e-02   1.847  0.06474 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 498.10  on 391  degrees of freedom\nResidual deviance: 344.02  on 383  degrees of freedom\nAIC: 362.02\n\nNumber of Fisher Scoring iterations: 5\n\n\nLe paramètre family = binomial indique que nous utilisons une régression logistique et que notre variable est binaire.\nNous nous concentrons sur la significativité statistique de chaque coefficient. En effet, la \\(p-valeur\\) nous indique la probabilité que l’effet de la variable explicative soit dû au hasard. Plus sa valeur est faible, plus l’effet de la variable explicative sur notre variable \\(Y\\) (Outcome) est statistiquement significatif. Au contraire, si elle est supérieure à 0.05, cela suggère que l’effet de la variable explicative sur notre variable \\(Y\\) n’est pas significatif et pourrait être dû au hasard.\nIci, les variables explicatives avec des p-valeurs inférieur à 5 % sont :\n\nGlucose (\\(p-valeur\\) = \\(3,24 * 10^{-11}\\)) : L’effet de la variable glucose sur la probabilité de la variable diabète est significatif.\nBMI (\\(p-valeur\\) = \\(0.00989\\)) : L’effet de la variable BMI sur la probabilité de la variable diabète est significatif.\nDiabetesPedigreeFunction (\\(p-valeur\\) = \\(0.00760\\)) : L’effet de la variable DiabetesPedigreeFunction sur la probabilité de la variable diabète est significatif.\n\nLes variables restantes ont des \\(p-valeur\\) &gt; \\(5 \\%\\), on en conclut donc qu’elles ne sont pas liées au risque de diabète.\n\n\nCalcul des odds-ratios et de leurs intervalles de confiance\nL’odds-ratio est une mesure statistique exprimant le degré de dépendance entre des variables aléatoires qualitatives. Il permet de mesurer l’effet d’un facteur en comparant les chances qu’un évènement se produise dans un groupe par rapport à un autre groupe.\nIci, l’odds-ratio va représenter l’impact de nos variables explicatives sur les chances que notre variable \\(Y\\) (Outcome) prenne la valeur diabétique (1) ou non-diabétique.\nNous calculons ci-dessous les odds-ratios de toutes les variables, ainsi que leurs intervalles de confiance qui nous permetteront d’être sur à 95 % que l’odds-ratio se trouve entre les bornes inférieurs et supérieurs.\nAfin d’obtenir les odds-ratios à partir des coefficients de régression logistique, nous avons pris l’exponentielle des coefficients.\n\n# Permet la realisation de tableau\nlibrary(knitr)\n\nkable(exp(model_complet$coefficients[-1]), col.names = c(\"Variable\", \"Odd-ratios\"))\n\n\n\n\nVariable\nOdd-ratios\n\n\n\n\nPregnancies\n1.0856289\n\n\nGlucose\n1.0390112\n\n\nBloodPressure\n0.9985807\n\n\nSkinThickness\n1.0112846\n\n\nInsulin\n0.9991750\n\n\nBMI\n1.0730849\n\n\nDiabetesPedigreeFunction\n3.1296107\n\n\nAge\n1.0345346\n\n\n\n\nkable(exp(confint(model_complet)[-1, ]), col.names = c(\"Variable\", \"IC_inf\", \"IC_sup\"))\n\n\n\n\nVariable\nIC_inf\nIC_sup\n\n\n\n\nPregnancies\n0.9743237\n1.211631\n\n\nGlucose\n1.0277173\n1.051303\n\n\nBloodPressure\n0.9757909\n1.022307\n\n\nSkinThickness\n0.9778466\n1.045780\n\n\nInsulin\n0.9966180\n1.001767\n\n\nBMI\n1.0178269\n1.133537\n\n\nDiabetesPedigreeFunction\n1.3783799\n7.368273\n\n\nAge\n0.9985446\n1.073523\n\n\n\n\n\nNous nous intéressons uniquement aux variables ayant un effet significatif sur la probabilité de la variable Outcome. Ce sont les variables Glucose, BMI et DiabetesPedigreeFunction.\nOn note ici qu’une augmentation de 20 de BMI correspond à \\(1.0730849^{20} = 4.09\\) plus de risque d’avoir du diabète que de ne pas en avoir. Une augmentation de 30 de glucose correspond à \\(1.0390112^{30} = 3.15\\) plus de risque d’avoir du diabète que de ne pas en avoir. Enfin, une augmentation de 2 de DiabetesPedigreeFunction correspond à \\(3.1296107^{2} = 9.79\\) plus de risque d’avoir du diabète que de ne pas en avoir.\nNous allons maintenant exhiber des profils d’individus particulièrement à risque d’être diabétiques à l’aide des coefficients de notre modèle de régression logistique.\nPar exemple, un individu particulièrement à risque d’avoir du diabète sera un individu ayant un taux de glucose égale à 160, 40 de BMI et 1.5 de DiabetesPedigreeFunction. Son risque de diabète est alors égal à environ 99.9 %.\n\nscore &lt;- exp(model_complet$coefficients[1]+\n             160*model_complet$coefficients[3]+\n             60*model_complet$coefficients[7]+\n             1.5*model_complet$coefficients[8])\nexp(score)/(1+exp(score))\n\n(Intercept) \n  0.9994916 \n\n\nCependant, si ce même individu avait eu un BMI de 20 au lieu de 40, son risque de diabète aurait grandement diminué (71.4 %).\n\nscore &lt;- exp(model_complet$coefficients[1]+\n             160*model_complet$coefficients[3]+\n             30*model_complet$coefficients[7]+\n             1.5*model_complet$coefficients[8])\nexp(score)/(1+exp(score))\n\n(Intercept) \n  0.7137806 \n\n\n\n\nMéthode de sélection de variable\nToutes les variables ne contribuent pas nécessairement à expliquer notre variable Y, nous allons voir comment sélectionner certaines variables afin de simplifier notre modèle, mais aussi pour améliorer la classification.\nIci, nous utiliserons l’AIC (Critère d’information d’Akaike) qui est une mesure de la qualité d’un modèle statistique. L’AIC va nous permettre de comparer les différents modèles en utilisant un critère de vraisemblance. Il représente un compromis entre le biais (qui diminue avec le nombre de paramètres) et la parcimonie (nécessité de décrire les données avec le plus petit nombre de paramètres possible).\n\\[AIC = -2 \\cdot \\log(L) + 2 \\cdot k\\]\n\n\\(L\\) est la vraisemblance maximisée\n\\(k\\) est le nombre de paramètres dans le modèle\n\\(-2 \\cdot \\log(L)\\) est la déviance du modèle. Elle pénalise par 2 fois le nombre de paramètres\n\nLa fonction step() de R, nous permet d’effectuer une sélection descendante. On commence par le modèle complet incluant toutes les variables explicatives, puis on retire progressivement les variables qui contribuent le moins à l’ajustement du modèle, jusqu’à obtenir un modèle optimal.\n\nmodel_final &lt;- step(model_complet)\n\nStart:  AIC=362.02\nOutcome ~ Pregnancies + Glucose + BloodPressure + SkinThickness + \n    Insulin + BMI + DiabetesPedigreeFunction + Age\n\n                           Df Deviance    AIC\n- BloodPressure             1   344.04 360.04\n- Insulin                   1   344.42 360.42\n- SkinThickness             1   344.45 360.45\n&lt;none&gt;                          344.02 362.02\n- Pregnancies               1   346.24 362.24\n- Age                       1   347.55 363.55\n- BMI                       1   350.89 366.89\n- DiabetesPedigreeFunction  1   351.58 367.58\n- Glucose                   1   396.95 412.95\n\nStep:  AIC=360.04\nOutcome ~ Pregnancies + Glucose + SkinThickness + Insulin + BMI + \n    DiabetesPedigreeFunction + Age\n\n                           Df Deviance    AIC\n- Insulin                   1   344.42 358.42\n- SkinThickness             1   344.46 358.46\n&lt;none&gt;                          344.04 360.04\n- Pregnancies               1   346.24 360.24\n- Age                       1   347.60 361.60\n- BMI                       1   351.28 365.28\n- DiabetesPedigreeFunction  1   351.67 365.67\n- Glucose                   1   397.31 411.31\n\nStep:  AIC=358.42\nOutcome ~ Pregnancies + Glucose + SkinThickness + BMI + DiabetesPedigreeFunction + \n    Age\n\n                           Df Deviance    AIC\n- SkinThickness             1   344.89 356.89\n&lt;none&gt;                          344.42 358.42\n- Pregnancies               1   346.74 358.74\n- Age                       1   347.87 359.87\n- BMI                       1   351.32 363.32\n- DiabetesPedigreeFunction  1   351.90 363.90\n- Glucose                   1   411.11 423.11\n\nStep:  AIC=356.89\nOutcome ~ Pregnancies + Glucose + BMI + DiabetesPedigreeFunction + \n    Age\n\n                           Df Deviance    AIC\n&lt;none&gt;                          344.89 356.89\n- Pregnancies               1   347.23 357.23\n- Age                       1   348.72 358.72\n- DiabetesPedigreeFunction  1   352.72 362.72\n- BMI                       1   360.44 370.44\n- Glucose                   1   411.85 421.85\n\n\nLe meilleur modèle est celui possédant l’AIC le plus faible.\nIci, le meilleur modèle inclut les variables Age, Pregnancies, BMI, DiabetesPedigreeFunction et Age.\nCi-dessous les résultats de notre modèle.\n\nsummary(model_final)\n\n\nCall:\nglm(formula = Outcome ~ Pregnancies + Glucose + BMI + DiabetesPedigreeFunction + \n    Age, family = \"binomial\", data = diabetes)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.8827  -0.6535  -0.3694   0.6521   2.5814  \n\nCoefficients:\n                          Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)              -9.992080   1.086866  -9.193  &lt; 2e-16 ***\nPregnancies               0.083953   0.055031   1.526 0.127117    \nGlucose                   0.036458   0.004978   7.324 2.41e-13 ***\nBMI                       0.078139   0.020605   3.792 0.000149 ***\nDiabetesPedigreeFunction  1.150913   0.424242   2.713 0.006670 ** \nAge                       0.034360   0.017810   1.929 0.053692 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 498.10  on 391  degrees of freedom\nResidual deviance: 344.89  on 386  degrees of freedom\nAIC: 356.89\n\nNumber of Fisher Scoring iterations: 5\n\n\nMis à part la variable Pregnancies, tous ont des \\(p-valeur\\) significative. Elles ont toutes un impact positif sur la probabilité du patient à avoir le diabète.\nMaintenant, nous calculons les odds-ratio de ce modèle.\n\n\n\n\n\nVariable\nOdd-ratios\n\n\n\n\nPregnancies\n1.087578\n\n\nGlucose\n1.037130\n\n\nBMI\n1.081273\n\n\nDiabetesPedigreeFunction\n3.161077\n\n\nAge\n1.034957\n\n\n\n\n\n\n\n\nVariable\nIC_inf\nIC_sup\n\n\n\n\nPregnancies\n0.9769517\n1.212971\n\n\nGlucose\n1.0274079\n1.047716\n\n\nBMI\n1.0395095\n1.127321\n\n\nDiabetesPedigreeFunction\n1.4016639\n7.398164\n\n\nAge\n0.9999761\n1.072664\n\n\n\n\n\n\n\nClassification avec le modèle de régression logistique\nNous allons maintenant classifier nos données à l’aide de notre modèle de régression logistique.\nDans un premier temps, nous allons séparer notre jeu de données en deux sous-échantillons.\n\nEchantillon d’apprentissage : Ce dernier contient 80 % de notre jeu de données. Notre modèle va apprendre à prédire sur cet échantillon.\nEchantillon de test : Cet échantillon contient les 20 % restants. Il va nous servir à tester notre modèle et à comparer les résultats avec les vraies valeurs de cet échantillon.\n\n\n# On fixe la graine (resultat reproductible)\nset.seed(75016)\n\n# Nombres d'observations dans notre jeu de donnees\nn &lt;- nrow(diabetes)\n\n# Nombres d'observations a selectionne dans l'echant d'apprentissage\nN &lt;- floor(n*0.8)\n\n# Selection des individus aleatoirement\nidx &lt;- sample(n, N, replace = F)\n\n# Echantillon d'apprentissage (80%)\ndataL &lt;- diabetes[idx,]\n\n# Echantillon de test (20%)\ndataV &lt;- diabetes[-c(idx),]\n\nOn entraîne notre modèle final (choisi précédemment à l’aide de l’AIC) sur l’échantillon d’apprentissage et on utilise la fonction predict qui permet de prédire pour tout individu de l’échantillon de test, sa probabilité d’être diabétique.\n\n# Entrainement du meilleur modele sur echantillon d'apprentissage (DataL)\nmodel_final_train &lt;- glm(Outcome ~ Pregnancies + Glucose + BMI + DiabetesPedigreeFunction + Age , data = dataL, family = \"binomial\")\n\n# Prediction de la probabilite d'etre diabetique sur echantillon de test (DataV)\nprediction_modele &lt;- predict(model_final_train,dataV, type = \"response\")\n\nNous calculons ensuite le taux de mauvaise classification moyen. Ce dernier est tout simplement une comparaison des classifications à leurs vraies étiquettes. On calcule ensuite le pourcentage de données mal classifiées.\nDans notre cas, nous allons créer une règle de classification. Si la prédiction est inférieure à 0.5 alors le patient prendre la valeur 0 (non-diabétique) dans le cas contraire, il prendra la valeur 1 (diabétique).\n\n# Regle de classification\nprediction_regle &lt;- ifelse(prediction_modele &gt;= 0.5, 1,0)\n\nerreur &lt;- mean(prediction_regle != dataV$Outcome)\npaste0(\"Le taux d'erreur moyen est de \", round(erreur,3)*100,\" %\")\n\n[1] \"Le taux d'erreur moyen est de 25.3 %\"\n\n\n\n\nComparaison de la régression logistique avec l’algorithme des k-plus proches voisins\nPrécédemment, nous avons réalisé une classification sur ce même jeu de données en utilisant l’algorithme des \\(k\\) plus proches voisins. La valeur de \\(k\\) optimal était \\(k\\) = 19, c’est-à-dire la valeur de \\(k\\) avec laquelle le taux de mauvaise classification était le plus faible.\nNous mettons donc en place notre algorithme des \\(k\\) plus proches voisins avec \\(k\\) = 19, et on réalise nos prédictions à l’aide de la fonction knn() du package class.\n\nprediction_knn &lt;- knn(train = dataL[,-9], test = dataV[,-9], cl = dataL[,9], k = 19, prob = F)\n# resultat des k plus proches voisins\nprediction_knn\n\n [1] 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0\n[39] 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 0 0 0 1 0 1 0\n[77] 0 0 0\nLevels: 0 1\n\n\nOn calcule ensuite notre taux d’erreur moyen.\n\nerreur_knn &lt;- mean(prediction_knn != dataV[,9])\npaste0(\"Le taux d'erreur moyen est de \", round(erreur_knn,3)*100, \" %\")\n\n[1] \"Le taux d'erreur moyen est de 20.3 %\"\n\n\nMaintenant, que nous avons réalisé nos prédictions avec nos deux méthodes, nous sommes en mesure de comparer les deux taux d’erreur moyens. Le taux d’erreur le plus bas est évidemment préférable. Ici, les taux d’erreur moyens sont proches bien que celui de la régression logistique est plus élevée (25.3 %) que celui de l’algorithme des \\(k\\) plus proches voisins (20.3 %).\n\n\nConclusion\nEnfin, nous allons faire varier le seuil utilisé dans le critère de classification et calculer la sensibilité et la spécificité pour chacune des valeurs du seuil possible.\n\n# Evaluation de 100 seuils differents\nl &lt;- 100\n\n# Predicition\npreds &lt;- predict(model_final_train, dataV, type = \"response\")\n\n# Sequence de seuils\nc &lt;- seq(1.001 * min(preds), 0.999 * max(preds), length.out = l)\n\n# Initialisation des vecteurs sensibilite et specificite\nSe &lt;- rep(NA,l)\nSp &lt;- rep(NA,l)\n\nfor (j in 1:l) {\n  mod.final.classif &lt;- (preds &gt;= c[j])  # Classe (TRUE ou FALSE) de la prédiction\n  pt &lt;- table(mod.final.classif, dataV$Outcome)  # Comparaison des classes prédites\n  \n  if (nrow(pt) &gt;= 2 && ncol(pt) &gt;= 2) {\n\n    Se[j] &lt;- prop.table(pt, margin=2)[2, 2]  # Sensibilité\n    Sp[j] &lt;- prop.table(pt, margin=2)[1, 1]  # Spécificité\n  } else {\n\n    Se[j] &lt;- NA\n    Sp[j] &lt;- NA\n  }\n}\n\npar(mfrow=c(1,2))\nplot(c,Se,main=\"Sensibilité\",type='s', col = \"orange\")\nplot(c,Sp,main=\"Specificité\",type='s', col = \"purple\")\n\n\n\n\n\n\n\n\nSur les deux graphiques ci-dessus, la courbe de sensibilité montre comment le taux de détection des diabétiques varie en fonction des différents seuils de classification. Tandis que la courbe de spécificité montre comment le taux de détection des non-diabétiques varie également en fonction des différents seuils de classification.\nOn cherche à maximiser la sensibilité et la spécificité. Cependant une augmentation de la sensibilité peut entraîner une diminution de la spécificité et vice versa.\nEn somme, ces deux mesures permettent de déterminer à quel point le modèle est efficace dans la classification des cas positifs et négatifs, et d’ajuster les seuils de décision en fonction des besoins spécifiques.\nPuis, nous pouvons calculer la courbe ROC. Elle sert à évaluer la performance d’un modèle de classification binaire et en particulier les modèles qui prédisent une probabilité. Elle représente la sensibilité en fonction de 1 – spécificité pour toutes les valeurs seuils possibles du marqueur étudié.\n\npar(mfrow=c(1,1))\nplot(1-Sp,Se,type='s',main=\"Courbe ROC\")\nabline(0,1,col='blue')\n\n\n\n\n\n\n\n\nNotre classification est plutôt bonne au vu de la courbe ROC. La courbe est assez proche du coin supérieur gauche du graphique, ce qui indique un taux élevé de vrais positifs (sensibilité). Notre modèle arrive assez bien à identifier les personnes diabétiques en minimisant les erreurs de classification des cas négatifs. De plus, la courbe est largement supérieure à la courbe 0,1."
  },
  {
    "objectID": "projets/fontaines_eau.html",
    "href": "projets/fontaines_eau.html",
    "title": "Où boire de l’eau à Paris ?",
    "section": "",
    "text": "Ce mini-projet utilise le langage R pour créer une carte interactive des points d’eau publics dans la ville de Paris. Il inclut également une application Shiny, permettant une exploration dynamique et interactive des données."
  },
  {
    "objectID": "projets/fontaines_eau.html#aperçu-de-lapplication",
    "href": "projets/fontaines_eau.html#aperçu-de-lapplication",
    "title": "Où boire de l’eau à Paris ?",
    "section": "1 Aperçu de l’application",
    "text": "1 Aperçu de l’application\n\n\n\n\n\nL’objectif principal de ce projet est de proposer une visualisation géographique claire et accessible des points d’eau répartis dans Paris, grâce à une interface conviviale développée avec Shiny. Les données exploitées proviennent de la Ville de Paris et sont accessibles sur le site data.gouv. Cette application, à la fois fluide et interactive, permet aux utilisateurs d’explorer facilement les différents points d’eau présents dans chaque arrondissement de la capitale."
  },
  {
    "objectID": "projets/fontaines_eau.html#accédez-à-lapplication",
    "href": "projets/fontaines_eau.html#accédez-à-lapplication",
    "title": "Où boire de l’eau à Paris ?",
    "section": "2 Accédez à l’application",
    "text": "2 Accédez à l’application\nAccéder à l’application ici\nRepo github"
  },
  {
    "objectID": "projets/projets_but/migration.html",
    "href": "projets/projets_but/migration.html",
    "title": "Migration de données : De SQL à NoSQL",
    "section": "",
    "text": "Ce projet vise à migrer des données d’un environnement SQL vers un environnement NoSQL. Concrètement, il s’agit de transférer les informations stockées dans une base de données relationnelle traditionnelle, où les données sont organisées en tables avec des relations fixes, vers une base de données NoSQL, qui offre une structure plus flexible adaptée aux données non structurées ou semi-structurées.\nNous travaillons avec les données d’une entreprise de voitures qui rencontre des problèmes avec sa base de données actuelle : les requêtes sont lentes et des défaillances serveur entraînent des pertes de données. Pour résoudre ces problèmes, nous avons décidé de passer à un environnement NoSQL. Cette technologie permet de stocker des données sous une forme non structurée, offrant ainsi plus de flexibilité et de performance. Cette migration vise à améliorer la performance des requêtes et à préparer l’infrastructure pour une croissance future.\nLe dépôt GitHub contenant le rapport complet du projet, ainsi que les requêtes SQL et NoSQL associées, est disponible ici."
  },
  {
    "objectID": "projets/projets_but/migration.html#base-de-données-relationnelle",
    "href": "projets/projets_but/migration.html#base-de-données-relationnelle",
    "title": "Migration de données : De SQL à NoSQL",
    "section": "Base de données relationnelle",
    "text": "Base de données relationnelle\nLa base de données relationnelle initiale contient des informations sur les véhicules, les clients, les commandes, les employés… La représentation des données est claire et bien organisée. Chaque table dispose de relations avec d’autres tables, ce qui permet de structurer efficacement les informations et de faciliter les requêtes complexes.\n\n\n\nSchéma relationnel de la bdd initial\n\n\nDans un premier temps, nous avons créé des requêtes SQL sur cette base de données. Ces requêtes serviront de tests pour évaluer le succès de la migration. Nous comparerons les résultats obtenus dans la base de données relationnelle avec ceux obtenus dans la base NoSQL pour vérifier l’intégrité et la performance de la migration.\n\n\n\nRequêtes SQL\n\n\nLa base de données est au format SQLite. Nous avons importé le module sqlite3 en Python pour établir la connexion et interagir avec la base. Ensuite, nous avons utilisé la bibliothèque Pandas et notamment sa fonction read_sql_query(), pour exécuter et lire les résultats des requêtes SQL."
  },
  {
    "objectID": "projets/projets_but/migration.html#algorithme-de-migration",
    "href": "projets/projets_but/migration.html#algorithme-de-migration",
    "title": "Migration de données : De SQL à NoSQL",
    "section": "Algorithme de migration",
    "text": "Algorithme de migration\nIl existe plusieurs types de bases de données NoSQL (Clé-valeur, Document, Colonne et Graphe), chacun adapté à ses propres cas d’usage et ayant ses propres avantages et inconvénients, notamment en termes de scalabilité et de flexibilité. Le choix dépend donc de plusieurs facteurs clés, comme la structure des données, les exigences de performance…\nEn ce qui nous concerne, nous pouvons réaliser une migration vers un environnement NoSQL, car l’entreprise dispose d’une grande quantité de données structurées en constante croissance. De plus, il est possible d’améliorer significativement les performances d’accès aux données en optimisant le traitement de données plus importantes et en réduisant le temps de latence. On souhaite donc une solution qui offre plus de flexibilité et évolutivité, tout en préservant l’intégrité des données de notre base initial.\nAprès mûre réflexion, le format document est celui s’adaptant le mieux à notre objectif. En effet, il permet de structurer naturellement les entités de manière hiérarchique. Par exemple, un client peut être représenté par un document contenant ses commandes, chaque commande incluant les produits associés. De plus, il offre une grande flexibilité, permettant de traiter différents types de données sans modifications complexes du schéma. Enfin, il permet une scalabilité horizontale grâce à la partition de document, c-à-d si les données augmentent, on peut facilement ajouter de nouveaux serveurs pour stocker et gérer plus de documents, sans tout restructurer.\n\n\n\nExemple d’une modélisation au format Document\n\n\nCe modèle présente quelques inconvénients, notamment des performances limitées pour les requêtes complexes ou les jointures entre documents. De plus, les mises à jour simultanées de documents imbriqués peuvent être plus difficiles à gérer.\nNous avons décidé de structurer nos données autour de quatres collections : customers, payments, orders et employees."
  },
  {
    "objectID": "projets/projets_but/migration.html#script-de-migration",
    "href": "projets/projets_but/migration.html#script-de-migration",
    "title": "Migration de données : De SQL à NoSQL",
    "section": "Script de migration",
    "text": "Script de migration\nAvant de développer un script de migration, nous avons d’abord établi un pseudo-algorithme dans l’objecitf de structurer et organiser la logique de notre programme.\nEnsuite, pour la migration de nos données, nous avons utilisé SQLite comme source et MongoDB comme destination, en exploitant les bibliothèques Python sqlite3, pymongo et pandas. Le processus inclut l’extraction des données de SQLite, leur transformation au format document compatible avec MongoDB, et leur insertion dans les collections appropriées."
  },
  {
    "objectID": "index.html#formation",
    "href": "index.html#formation",
    "title": "Rachid SAHLI",
    "section": "Formation",
    "text": "Formation\n\n\n\n\n\n\nIUT de Paris - Rives de Seine (Université Paris Cité) | Paris, France\n\n\n\nBachelor universitaire de technologie en Science des données | Sept 2022 - Juin 2025\nCours suivis : Algèbre, Statistique, Probabilités, Programmation, Base de données, Économie"
  },
  {
    "objectID": "index.html#expérience",
    "href": "index.html#expérience",
    "title": "Rachid SAHLI",
    "section": "Expérience",
    "text": "Expérience\n\n\n\n\n\n\nInstitut national de la statistique et des études économiques (INSEE) | Montrouge, France\n\n\n\nProgrammeur Statistique | 2023 - 2025 Au sein de la Direction des statistiques démographiques et sociales, j’ai mené des travaux d’appariement de données administratives. L’objectif de ces travaux était de mesurer et de comparer la couverture de deux bases de sondages. Par ailleurs, j’ai comparé différents algorithmes d’appariement à l’aide d’analyses statistiques.\nMes missions ont inclus la manipulation de données brutes, le nettoyage des données, l’appariement de grands volumes de données, ainsi que leur analyse statistique."
  },
  {
    "objectID": "index.html#iut-de-paris---rives-de-seine-université-paris-cité-paris-france",
    "href": "index.html#iut-de-paris---rives-de-seine-université-paris-cité-paris-france",
    "title": "Rachid SAHLI",
    "section": "IUT de Paris - Rives de Seine (Université Paris Cité) | Paris, France",
    "text": "IUT de Paris - Rives de Seine (Université Paris Cité) | Paris, France\nBachelor universitaire de technologie en Science des données | Sept 2022 - Juin 2025\nCours suivis : Algèbre, Statistique, Probabilités, Programmation, Base de données, Économie"
  },
  {
    "objectID": "index.html#institut-national-de-la-statistique-et-des-études-économiques-insee-montrouge-france",
    "href": "index.html#institut-national-de-la-statistique-et-des-études-économiques-insee-montrouge-france",
    "title": "Rachid SAHLI",
    "section": "Institut national de la statistique et des études économiques (INSEE) | Montrouge, France",
    "text": "Institut national de la statistique et des études économiques (INSEE) | Montrouge, France\nStatisticien | Sept 2023 - Sept 2025 Au sein de la Direction des statistiques démographiques et sociales, j’ai mené des travaux d’appariement de données administratives. L’objectif de ces travaux était de mesurer et de comparer la couverture de deux bases de sondages. Par ailleurs, j’ai comparé différents algorithmes d’appariement à l’aide d’analyses statistiques.\nMes missions ont inclus la manipulation de données brutes, le nettoyage des données, l’appariement de grands volumes de données, ainsi que leur analyse statistique."
  },
  {
    "objectID": "projets/projets_but/essai_clinique.html",
    "href": "projets/projets_but/essai_clinique.html",
    "title": "Etude statistique dans un essai clinique",
    "section": "",
    "text": "Introduction\nCe projet consiste en la réalisation d’une étude statistique dans le cadre d’un essai clinique. Nous travaillons sur un jeu de données simulées pour effectuer l’analyse statistique d’une étude de phase 3. L’objectif est de fournir au laboratoire une Autorisation de Mise sur le Marché (AMM) avec une indication dans la prise en charge de la drépanocytose.\nLa drépanocytose est une maladie génétique qui affecte les globules rouges et peut entraîner des complications graves. Elle se manifeste notamment par une anémie, des crises douloureuses et un risque accru d’infections.\n\n\n\nSchéma de la drépanocytose\n\n\nLors d’une étude de phase 3, les chercheurs comparent un nouveau traitement prometteur au traitement standard, qui est le traitement reconnu et généralement administré pour une affection ou une maladie. Dans notre étude, ce traitement s’appelle le Voxelotor.\nNotre objectif est d’évaluer l’effet du voxelotor en mesurant l’amélioration du taux d’hémoglobine chez les patients, comparé à un placebo. Durant notre étude statistique, nous testerons diverses hypothèses et utiliserons des méthodes statistiques pour évaluer l’efficacité de ce traitement.\nLe rapport peut être consulté ici. Vous trouverez également le code R ici.\n\n\nMéthodologie\n\n\nRésultat\n\n\nConclusion"
  },
  {
    "objectID": "projets/projets_but/serie_temp_charbon.html",
    "href": "projets/projets_but/serie_temp_charbon.html",
    "title": "Série Temporelle : Production de charbon aux États-Unis",
    "section": "",
    "text": "Introduction\n\n\nMéthodologie\n\n\nRésultat"
  }
]